{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import ee\n",
    "import folium\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "GPU_AFFINTY  = 0\n",
    "GPU_MEMORY_LIMIT_GB =8 # For UNET use 8GB\n",
    "\n",
    "logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)\n",
    "# GPU_AFFINTY  = 0 #GeForce RTX 4090\n",
    "\n",
    "gpu_dict = {'4090':{'GPU_AFFINTY' : 0, 'GPU_MEMORY_LIMIT_GB':16}, \n",
    "            '2070':{'GPU_AFFINTY':1, 'GPU_MEMORY_LIMIT_GB':8}}\n",
    "\n",
    "\n",
    "sel_gpu = '4090'\n",
    "GPU_AFFINTY  = gpu_dict[sel_gpu]['GPU_AFFINTY'] #GeForce RTX 2070\n",
    "GPU_MEMORY_LIMIT_GB =gpu_dict[sel_gpu]['GPU_MEMORY_LIMIT_GB']\n",
    "\n",
    "try:\n",
    " # ee.Authenticate()\n",
    " ee.Initialize()\n",
    "except:\n",
    " ee.Authenticate()\n",
    " ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "print('Tensorflow Version:',tf.__version__)\n",
    "print('Folium Version:',folium.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[GPU_AFFINTY], 'GPU')\n",
    "    GPU_MEMORY_LIMIT_GB = GPU_MEMORY_LIMIT_GB * 1e3\n",
    "    if GPU_MEMORY_LIMIT_GB == 0:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        tf.config.set_logical_device_configuration(gpus[GPU_AFFINTY],[tf.config.LogicalDeviceConfiguration(memory_limit=GPU_MEMORY_LIMIT_GB)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def creatDirectory(new_folder):\n",
    "    if not os.path.exists(new_folder):\n",
    "        print(f'lets make the directory: {new_folder}')\n",
    "        os.makedirs(new_folder)\n",
    "    else: return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENV Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swir1', 'nir', 'red', 'green', 'MNDWI', 'NDVI']\n"
     ]
    }
   ],
   "source": [
    "VERSION        = '2_BR'\n",
    "MOSAIC_VERSION = '4'\n",
    "BUCKET = 'mineracao_br'\n",
    "GDRIVE = 'mb9-unet-mining-brazil'\n",
    "FOLDER = 'training_samples'\n",
    "TRAINING_BASE = 'training_patches_'+MOSAIC_VERSION\n",
    "EVAL_BASE     = 'eval_patches_'+MOSAIC_VERSION\n",
    "\n",
    "#Local paths\n",
    "LOCAL_PATH  = '/mnt/storage10.2/mb9-unet-mineracao'\n",
    "MODEL_DIR   = LOCAL_PATH+'/checkpoint/v'+VERSION\n",
    "NFS_PATH = '/mnt/nfs/assets/Mapbiomas/modelos/mb9-unet-mineracao'\n",
    "OUTPUT_PATH = NFS_PATH+'/output/v'+VERSION\n",
    "GRID_PATH = \"/mnt/nfs/assets/Mapbiomas/GRID-ALLCALSSES-COL9.geojson\"\n",
    "\n",
    "creatDirectory(MODEL_DIR)\n",
    "creatDirectory(OUTPUT_PATH)\n",
    "\n",
    "# Exportation Configs\n",
    "BUCKET_patch = BUCKET\n",
    "FOLDER_patch = 'allPatch'\n",
    "FOLDER_classification = 'mb9_mining_'+VERSION\n",
    "\n",
    "# Specify inputs (Landsat bands) to the model and the response variable.\n",
    "opticalBands = ['swir1', 'nir', 'red','green','MNDWI','NDVI'] #['swir1', 'nir', 'red','NDVI','MNDWI'] DEFAULT\n",
    "BANDS    = opticalBands\n",
    "RESPONSE = 'supervised'\n",
    "FEATURES = BANDS + [RESPONSE]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE  = 256\n",
    "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "TRAIN_SIZE = 0\n",
    "EVAL_SIZE = 0\n",
    "\n",
    "print(BANDS)\n",
    "# Specify model training parameters.\n",
    "BATCH_SIZE  = 10\n",
    "DROPOUT     = 0.3 #0.5\n",
    "EPOCHS      = 50\n",
    "BUFFER_SIZE = 1000\n",
    "OPTIMIZER   = 'Nadam' \n",
    "LOSS        = 'BinaryCrossentropy'\n",
    "METRICS     = ['RootMeanSquaredError']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Visualization - MG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mosaic_year   = 2023\n",
    "\n",
    "# 3_MG E 4_MG\n",
    "# supervisedImg = ee.Image('projects/solved-mb9/assets/Mining/supervised-2023-2-MG').gte(1).unmask(0).rename(RESPONSE);\n",
    "\n",
    "# 5_MG: 2023 plus 1989 samples\n",
    "supervisedImg = ee.Image('projects/solved-mb9/assets/Mining/supervised-2023-6-MG').gte(1).unmask(0).rename(RESPONSE);\n",
    "#supervisedImg = ee.Image('projects/solved-mb9/assets/Mining/supervised-1990-4-MG').gte(1).unmask(0).rename(RESPONSE);\n",
    "\n",
    "supervisedChannel = supervisedImg.toByte().rename(RESPONSE);\n",
    "\n",
    "image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA6/mosaic_'+str(mosaic_year)).addBands(supervisedChannel)\n",
    "mapid = image.getMapId({'bands': ['swir1', 'nir', 'red'], 'min': 30, 'max': 150})\n",
    "map = folium.Map(location=[-5.9442, -56.5265])\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Planet',\n",
    "    overlay=True,\n",
    "    name='Mosaic composite',\n",
    "  ).add_to(map)\n",
    "mapid = supervisedChannel.select(RESPONSE).getMapId({'min': 0, 'max': 1, 'pallete':'#ff0000'})\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='supervisedLayer',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureStack = ee.Image.cat([\n",
    "  image.select(BANDS).unmask(0),\n",
    "  image.select(RESPONSE).unmask(0)\n",
    "]).float()\n",
    "\n",
    "list = ee.List.repeat(1, KERNEL_SIZE)\n",
    "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
    "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
    "\n",
    "arrays = featureStack.neighborhoodToArray(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainingPolys = ee.FeatureCollection('projects/solvedltda/assets/MB7_mining/trainPoly_unet_mb7_5')\n",
    "# evalPolys = ee.FeatureCollection('projects/solvedltda/assets/MB7_mining/testPoly_unet_mb7_5')\n",
    "\n",
    "\n",
    "# FOR MG MB9\n",
    "# trainingPolys3 = ee.FeatureCollection('projects/solved-mb9/assets/Mining/train_mg_v2')\n",
    "# trainingPolys4 = ee.FeatureCollection('projects/solved-mb9/assets/Mining/train_mg_v3')\n",
    "# trainingPolys = trainingPolys3.merge(trainingPolys4)\n",
    "trainingPolys = ee.FeatureCollection('projects/solved-mb9/assets/Mining/train_mg_v4')\n",
    "evalPolys = ee.FeatureCollection('projects/solved-mb9/assets/Mining/test_mg_v4')\n",
    "print(trainingPolys.size().getInfo())\n",
    "print(evalPolys.size().getInfo())\n",
    "\n",
    "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
    "polyImage = polyImage.updateMask(polyImage)\n",
    "\n",
    "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
    "map = folium.Map(location=[-1.3621, -45.2738], zoom_start=5)\n",
    "\n",
    "\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='training polygons',\n",
    "  ).add_to(map)\n",
    "mapid = supervisedChannel.select(RESPONSE).getMapId({'min': 0, 'max': 1, 'pallete':'#ff0000'})\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='supervisedLayer',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING_BASE= 'training_patches_1_6_MG_2023_FOR_MB9'\n",
    "# TRAINING_BASE\n",
    "# EVAL_BASE\n",
    "EVAL_BASE = 'eval_patches_1_6_MG_2023_FOR_MB9'\n",
    "FOLDER_TRAIN = 'training_samples_FOR_MB9_MG_2023'\n",
    "FOLDER_EVAL = 'eval_samples_FOR_MB9_BR_2023'\n",
    "GDRIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train/Test Chips Exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the feature collections to lists for iteration.\n",
    "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
    "evalPolysList = evalPolys.toList(evalPolys.size())\n",
    "# These numbers determined experimentally.\n",
    "n = 20 # Number of shards in each polygon.\n",
    "N = 200 # Total sample size in each polygon.\n",
    "\n",
    "#Add some generalism\n",
    "TRAIN_SIZE = trainingPolys.size().getInfo()*N\n",
    "EVAL_SIZE = evalPolys.size().getInfo()*N\n",
    "print('TRAIN:'+str(TRAIN_SIZE))\n",
    "print('EVAL:'+str(EVAL_SIZE))\n",
    "\n",
    "# Export all the training data (in many pieces), with one task \n",
    "# per geometry.\n",
    "for g in range(trainingPolys.size().getInfo()):\n",
    "  geomSample = ee.FeatureCollection([])\n",
    "  for i in range(n):\n",
    "    sample = arrays.sample(\n",
    "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
    "      scale = 30, \n",
    "      numPixels = N / n, # Size of the shard.\n",
    "      seed = i,\n",
    "      tileScale = 8\n",
    "    )\n",
    "    geomSample = geomSample.merge(sample)\n",
    "  \n",
    "  desc = TRAINING_BASE + '_g' + str(g)\n",
    "  task = ee.batch.Export.table.toDrive(\n",
    "    collection = geomSample,\n",
    "    description = desc, \n",
    "    folder = GDRIVE+'/'+FOLDER_TRAIN, \n",
    "    fileNamePrefix = desc,\n",
    "    fileFormat = 'TFRecord',\n",
    "    selectors = BANDS + [RESPONSE]\n",
    "  )\n",
    "  task.start()\n",
    "\n",
    "# Export all the evaluation data.\n",
    "for g in range(evalPolys.size().getInfo()):\n",
    "  geomSample = ee.FeatureCollection([])\n",
    "  for i in range(n):\n",
    "    sample = arrays.sample(\n",
    "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
    "      scale = 30, \n",
    "      numPixels = N / n,\n",
    "      seed = i,\n",
    "      tileScale = 8\n",
    "    )\n",
    "    geomSample = geomSample.merge(sample)\n",
    "  \n",
    "  desc = EVAL_BASE + '_g' + str(g)\n",
    "  task = ee.batch.Export.table.toDrive(\n",
    "    collection = geomSample,\n",
    "    description = desc, \n",
    "    folder = GDRIVE+'/'+FOLDER_EVAL, \n",
    "    fileNamePrefix = desc,\n",
    "    fileFormat = 'TFRecord',\n",
    "    selectors = BANDS + [RESPONSE],\n",
    "  )\n",
    "  task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "  Returns: \n",
    "    A dictionary of tensors, keyed by feature name.\n",
    "  \"\"\"\n",
    "  print(FEATURES_DICT)\n",
    "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "\n",
    "\n",
    "def to_tuple(inputs):\n",
    "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
    "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
    "  Args:\n",
    "    inputs: A dictionary of tensors, keyed by feature name.\n",
    "  Returns: \n",
    "    A dtuple of (inputs, outputs).\n",
    "  \"\"\"\n",
    "  inputsList = [inputs.get(key) for key in FEATURES]\n",
    "  stacked = tf.stack(inputsList, axis=0)\n",
    "  # Convert from CHW to HWC\n",
    "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
    "\n",
    "\n",
    "def get_dataset(pattern):\n",
    "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
    "  Get all the files matching the pattern, parse and convert to tuple.\n",
    "  Args:\n",
    "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
    "  Returns: \n",
    "    A tf.data.Dataset\n",
    "  \"\"\"\n",
    "  glob = tf.io.gfile.glob(pattern)\n",
    "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    \"\"\"Get the preprocessed training dataset\n",
    "  Returns: \n",
    "    A tf.data.Dataset of training data.\n",
    "  \"\"\"\n",
    "    # TRAINING_BASE = \"training_patches_4\"\n",
    "    \n",
    "    # Novas amostras na train_2\n",
    "    # glob = '/mnt/storage4/modelos/mb8-unet-mineracao/train/' + TRAINING_BASE + '*'\n",
    "    \n",
    "    # glob = '/mnt/storage4/modelos/mb7-unet-mineracao/train/' + TRAINING_BASE + '*'\n",
    "    \n",
    "    # MB9 BRASIL\n",
    "    # TRAINING_BASE = \"training_patches_1_FOR_MB9_BR_2022_FROM_MB8\"\n",
    "    # glob = '/mnt/storage10.2/mb9-unet-mineracao/train/v1_BR/' + TRAINING_BASE + '*'\n",
    "    # glob = '/mnt/storage10.2/mb9-unet-mineracao/train/test44/' + TRAINING_BASE + '*'\n",
    "    \n",
    "    # MB9 MG\n",
    "    TRAINING_BASE = \"training_patches_1\"\n",
    "    # glob = '/mnt/storage10.2/mb9-unet-mineracao/train/v5_MG/' + TRAINING_BASE + '*'\n",
    "    glob = '/mnt/nfs/assets/Mapbiomas/samples/mb9-unet-mining/v6/train/'+TRAINING_BASE + '*'\n",
    "    print(glob)\n",
    "    dataset = get_dataset(glob)\n",
    "    train_size = dataset.reduce(np.int64(0), lambda x,_ : x + 1).numpy()\n",
    "    print(train_size)  # v1_BR =>42605 | v1_BR_180_TF1 => 46605\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "training = get_training_dataset()\n",
    "\n",
    "# print(iter(training.take(1)).next())\n",
    "# 42605\n",
    "print(training.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
    "evalPolysList = evalPolys.toList(evalPolys.size())\n",
    "# These numbers determined experimentally.\n",
    "n = 20 # Number of shards in each polygon.\n",
    "N = 200 # Total sample size in each polygon.\n",
    "\n",
    "#Add some generalism\n",
    "TRAIN_SIZE = trainingPolys.size().getInfo()*N*2\n",
    "EVAL_SIZE = evalPolys.size().getInfo()*N*2\n",
    "print('TRAIN:'+str(TRAIN_SIZE))\n",
    "print('EVAL:'+str(EVAL_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_eval_dataset():\n",
    "    # EVAL_BASE = 'eval_patches_4'\n",
    "    # Novas amostras na eval_2\n",
    "    # glob = '/mnt/storage4/modelos/mb7-unet-mineracao/eval/' + EVAL_BASE + '*'\n",
    "    \n",
    "    \n",
    "    # EVAL_BASE = 'eval_patches_4'\n",
    "    # glob = '/mnt/storage4/modelos/mb7-unet-mineracao/eval/' + EVAL_BASE + '*'\n",
    "    \n",
    "    \n",
    "    # MB9 BRASIL\n",
    "    # EVAL_BASE = \"eval_patches_1_FOR_MB9_BR_2022_FROM_MB8\"\n",
    "    # glob = '/mnt/storage10.2/mb9-unet-mineracao/eval/v1_BR/' + EVAL_BASE + '*'\n",
    "    \n",
    "    # MB9 MG\n",
    "    EVAL_BASE = 'eval_patches_1'\n",
    "    glob = '/mnt/nfs/assets/Mapbiomas/samples/mb9-unet-mining/v6/eval/' + EVAL_BASE + '*'\n",
    "    \n",
    "    print(glob)\n",
    "    dataset = get_dataset(glob)\n",
    "    eval_size = dataset.reduce(np.int64(0), lambda x,_ : x + 1).numpy()\n",
    "    print(eval_size)  # v2 => 600 | v3_mg => 1400\n",
    "    # dataset = dataset.batch(10).repeat()\n",
    "    dataset = dataset.batch(1).repeat()\n",
    "    return dataset\n",
    "\n",
    "evaluation = get_eval_dataset()\n",
    "# print(iter(evaluation.take(1)).next())\n",
    "# 22420\n",
    "print(evaluation.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\treturn encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "\tencoder = conv_block(input_tensor, num_filters)\n",
    "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "\treturn encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\treturn decoder\n",
    "\n",
    "def get_model():\n",
    "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256 (shape=[256, 256, len(BANDS)\n",
    "\tencoder0_pool, encoder0 = encoder_block(inputs, 64) # 128\n",
    "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 128) # 64\n",
    "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 256) # 32\n",
    "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 512) # 16\n",
    "\tcenter = conv_block(encoder3_pool, 1024) # 8 center\n",
    "\tdecoder4 = decoder_block(center, encoder3, 512) # 16\n",
    "\tdecoder3 = decoder_block(decoder4, encoder2, 256) # 32\n",
    "\tdecoder2 = decoder_block(decoder3, encoder1, 128) # 64\n",
    "\tdecoder1 = decoder_block(decoder2, encoder0, 64) # 128\n",
    "\tdropout = layers.Dropout(DROPOUT, name=\"dropout\", noise_shape=None, seed=None)(decoder1)\n",
    "\toutputs = layers.Conv2D(1, (1, 1),  activation=tf.nn.sigmoid, padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(dropout)\n",
    "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\toptimizer = tf.keras.optimizers.Adam( 0.000005, name='optimizer')\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=optimizer, \n",
    "\t\tloss=losses.get(LOSS),\n",
    "\t\tmetrics=[metrics.get(metric) for metric in METRICS]\n",
    "    )\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Selection/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 6)]      0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, None, None, 64)       3520      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, None, None, 64)       256       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, None, None, 64)       0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, None, None, 64)       36928     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, None, None, 64)       256       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, None, None, 64)       0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, None, None, 64)       0         ['activation_1[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, None, None, 128)      73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, None, None, 128)      512       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, None, None, 128)      0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, None, None, 128)      147584    ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, None, None, 128)      512       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, None, None, 128)      0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, None, None, 128)      0         ['activation_3[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, None, None, 256)      295168    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, None, None, 256)      1024      ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, None, None, 256)      0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, None, None, 256)      590080    ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, None, None, 256)      1024      ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, None, None, 256)      0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, None, None, 256)      0         ['activation_5[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, None, None, 512)      1180160   ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, None, None, 512)      2048      ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, None, None, 512)      0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, None, None, 512)      2359808   ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, None, None, 512)      2048      ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, None, None, 512)      0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, None, None, 512)      0         ['activation_7[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, None, None, 1024)     4719616   ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, None, None, 1024)     4096      ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, None, None, 1024)     0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, None, None, 1024)     9438208   ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, None, None, 1024)     4096      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, None, None, 1024)     0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, None, None, 512)      2097664   ['activation_9[0][0]']        \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, None, 1024)     0         ['activation_7[0][0]',        \n",
      "                                                                     'conv2d_transpose[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, None, None, 1024)     4096      ['concatenate[0][0]']         \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, None, None, 1024)     0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, None, None, 512)      4719104   ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, None, None, 512)      2048      ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, None, None, 512)      0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, None, None, 512)      2359808   ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, None, None, 512)      2048      ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, None, None, 512)      0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, None, None, 256)      524544    ['activation_12[0][0]']       \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, None, None, 512)      0         ['activation_5[0][0]',        \n",
      " )                                                                   'conv2d_transpose_1[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, None, None, 512)      2048      ['concatenate_1[0][0]']       \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, None, None, 512)      0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, None, None, 256)      1179904   ['activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, None, None, 256)      1024      ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, None, None, 256)      0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, None, None, 256)      590080    ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, None, None, 256)      1024      ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, None, None, 256)      0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, None, None, 128)      131200    ['activation_15[0][0]']       \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, None, None, 256)      0         ['activation_3[0][0]',        \n",
      " )                                                                   'conv2d_transpose_2[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, None, None, 256)      1024      ['concatenate_2[0][0]']       \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, None, None, 256)      0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, None, None, 128)      295040    ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, None, None, 128)      512       ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, None, None, 128)      0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, None, None, 128)      147584    ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, None, None, 128)      512       ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, None, None, 128)      0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, None, None, 64)       32832     ['activation_18[0][0]']       \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, None, None, 128)      0         ['activation_1[0][0]',        \n",
      " )                                                                   'conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, None, None, 128)      512       ['concatenate_3[0][0]']       \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, None, None, 128)      0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, None, None, 64)       73792     ['activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, None, None, 64)       256       ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, None, None, 64)       0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, None, None, 64)       36928     ['activation_20[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, None, None, 64)       256       ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, None, None, 64)       0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, None, None, 64)       0         ['activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, None, None, 1)        65        ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 31064705 (118.50 MB)\n",
      "Trainable params: 31049089 (118.44 MB)\n",
      "Non-trainable params: 15616 (61.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "#from tensorflow.python.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "#import pydot\n",
    "#import graphviz\n",
    "m = get_model() # UNET\n",
    "#m = FCN_DK3(2, 256, 256)# FCN-DK\n",
    "#Image(model_to_dot(m, show_shapes=True, show_layer_names=True, rankdir='TB',expand_nested=False, dpi=60, subgraph=False).create(prog='dot',format='png'))\n",
    "\n",
    "\n",
    "\n",
    "# =========================== UNET TOTAL BRAZIL (UNRELIABLE) ===========================\n",
    "# EPOCH = 194#\n",
    "# CHECK_MODEL_DIR = '/mnt/storage4/modelos/mb7-unet-mineracao/checkpoint/v8/cp-00.ckpt/cp-0100.ckpt/cp-0150.ckpt/cp-0'+str(EPOCH)+'.ckpt'\n",
    "# #CHECK_MODEL_DIR = MODEL_DIR+'/cp-0'+str(EPOCH)+'.ckpt' \n",
    "# #print(CHECK_MODEL_DIR)\n",
    "# m.load_weights(CHECK_MODEL_DIR)\n",
    "# print(m.summary())\n",
    "\n",
    "\n",
    "# =========================== UNET MG MB9 ===========================\n",
    "#EPOCH = 60# FEITO\n",
    "#EPOCH = 30# FEITO\n",
    "#EPOCH = 20# FEITO\n",
    "# EPOCH = 98# FEITO\n",
    "EPOCH = 44# FEITO\n",
    "# EPOCH = 54# \n",
    "# CHECK_MODEL_DIR_MG = '/mnt/storage10.2/mb9-unet-mineracao/checkpoint/v7_MG/cp-00'+str(EPOCH)+'.ckpt'\n",
    "\n",
    "# # CHECK_MODEL_DIR = MODEL_DIR+'/cp-0'+str(EPOCH)+'.ckpt' \n",
    "# # creatDirectory(CHECK_MODEL_DIR_MG)\n",
    "# m.load_weights(CHECK_MODEL_DIR_MG)\n",
    "# print(m.summary())\n",
    "\n",
    "\n",
    "# =========================== UNET TOTAL BRAZIL MB9 ===========================\n",
    "# EPOCH = 0\n",
    "# MODEL_DIR = '/mnt/storage10.2/mb9-unet-mineracao/checkpoint/v1_BR/cp-0'+str(EPOCH)+'.ckpt'\n",
    "# creatDirectory(MODEL_DIR)\n",
    "# # m.load_weights(CHECK_MODEL_DIR_MG)\n",
    "# print(m.summary())\n",
    "\n",
    "\n",
    "# =========================== UNET TOTAL BRAZIL MB9 ===========================\n",
    "EPOCH = 170\n",
    "MODEL_DIR = '/mnt/storage10.2/mb9-unet-mineracao/checkpoint/v2_BR/cp-00.ckpt/cp-0100.ckpt/cp-0150.ckpt/cp-0'+str(EPOCH)+'.ckpt'\n",
    "# creatDirectory(MODEL_DIR)\n",
    "m.load_weights(MODEL_DIR)\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime, os\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "checkpoint_path = MODEL_DIR+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
    "#evalPolysList = evalPolys.toList(evalPolys.size())\n",
    "n = 20 # Number of shards in each polygon.\n",
    "N = 200 # Total sample size in each polygon.\n",
    "#TRAIN_SIZE = trainingPolys.size().getInfo()*N\n",
    "#EVAL_SIZE = evalPolys.size().getInfo()*N\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='output/v'+VERSION+'/log_model',write_images=True)\n",
    "\n",
    "cp_callback  = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,verbose=1, save_weights_only=True, period=2)\n",
    "m.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "\n",
    "result = m.fit(x=training,\n",
    "  epochs=100,\n",
    "  initial_epoch=0, # REMEBER TO CHANGE THIS INITIAL EPOCH PARAM, WHEN OTHER MODEL HAS BEEN LOADED\n",
    "  steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "  verbose=1,\n",
    "  shuffle=True,\n",
    "  validation_data=evaluation,\n",
    "  validation_steps=19200,#500,\n",
    "  callbacks = [cp_callback]) #tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doExport(out_image_base,index_in, kernel_buffer, roi):\n",
    "  \"\"\"Run the image export task.  Block until complete.\n",
    "  \"\"\"\n",
    "  index = index_in\n",
    "  image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA6/mosaic_'+str(index))\n",
    "  out_image_base2 = out_image_base+'_'+str(index_in)\n",
    "\n",
    "  task = ee.batch.Export.image.toDrive(\n",
    "    image = image.select(BANDS).toFloat(),\n",
    "    description = out_image_base+'_'+str(index),\n",
    "    folder        = outputBucket,\n",
    "    fileNamePrefix = 'mosaico_mb9_append_landsat/'+str(index)+'/'+out_image_base+'_'+str(index), \n",
    "    scale = 30,\n",
    "    region = roi,\n",
    "    fileFormat = 'TFRecord',\n",
    "    formatOptions = { \n",
    "      'patchDimensions': KERNEL_SHAPE,\n",
    "      'kernelSize': kernel_buffer,\n",
    "      'compressed': True,\n",
    "      'maxFileSize': 157286400\n",
    "    },\n",
    "    maxPixels=1e13\n",
    "  )\n",
    "\n",
    "  task.start()  \n",
    " \n",
    "  # Block until the task completes.\n",
    "  print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  #while task.active():\n",
    "  time.sleep(1)\n",
    "\n",
    "  # Error condition\n",
    "  if task.status()['state'] != 'COMPLETED':\n",
    "    print('Error with image export.')\n",
    "  else:\n",
    "    print('Image export completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VERSION     = '2_BR_append_4'\n",
    "LOCAL_PATH  = '/mnt/storage10.2/mb9-unet-mineracao'\n",
    "OUTPUT_PATH =  LOCAL_PATH+'/output/v'+VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/storage10.2/mb9-unet-mineracao/output/v2_BR_append_4\n",
      "/mnt/nfs/assets/Mapbiomas/modelos/mb9-unet-mineracao/output/v2_BR_append_4\n",
      "2_BR_append_4\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "print(OUTPUT_PATH)\n",
    "OUTPUT_PATH_NFS = '/mnt/nfs/assets/Mapbiomas/modelos/mb9-unet-mineracao/output/v2_BR_append_4'\n",
    "print(OUTPUT_PATH_NFS)\n",
    "# VERSION = '1_BR'\n",
    "print(VERSION)\n",
    "# VERSION = '1_BR_apped_2'\n",
    "OUTPUT_PATH = OUTPUT_PATH_NFS\n",
    "# EPOCH = 200\n",
    "print(EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from osgeo import ogr\n",
    "from osgeo import gdal\n",
    "import json\n",
    "def doPrediction(out_image_base,index_in,region_id, user_folder, kernel_buffer, region,epochs):\n",
    "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
    "  \"\"\"\n",
    "  out_image_base = out_image_base+'_'+str(index_in)\n",
    "  print(out_image_base)\n",
    "  filesList = glob.glob(\"/mnt/storage14/mosaics/\"+str(index_in)+\"/\"+out_image_base+\"*\")\n",
    "  rasterFolder = f'{OUTPUT_PATH}/classifications_tiff/{index_in}'\n",
    "  rasterURILZW = f'{rasterFolder}/outimage_{VERSION}_{region_id}_{index_in}_{epochs}epochs_lzw.tif'\n",
    "  \n",
    "\n",
    "  if os.path.exists(rasterURILZW):\n",
    "        print('Arquivo já predito \\n\\n')\n",
    "        return None\n",
    "    \n",
    "  if(len(filesList) == 0):\n",
    "    print('Sem arquivos')\n",
    "    return None\n",
    "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
    "  imageFilesList = []\n",
    "  jsonFile = None\n",
    "  for f in exportFilesList:\n",
    "    if f.endswith('.tfrecord.gz'):\n",
    "      imageFilesList.append(f)\n",
    "    elif f.endswith('.json'):\n",
    "      jsonFile = f\n",
    "  \n",
    "  imageFilesList.sort()\n",
    "  out_image_asset = user_folder + '/' + out_image_base\n",
    "\n",
    "  try: \n",
    "     jsonText = open(jsonFile)\n",
    "     mixer = json.load(jsonText)\n",
    "  except:\n",
    "     print(f'Grid com problema:{region_id} para o ano:{index_in} \\n\\n')\n",
    "     return None  \n",
    "  \n",
    "  \n",
    "  patches = mixer['totalPatches']\n",
    "  cols = int(mixer[\"patchesPerRow\"])\n",
    "  rows = int(mixer[\"totalPatches\"]/cols)\n",
    "  \n",
    "  # Get set up for prediction.\n",
    "  x_buffer = int(kernel_buffer[0] / 2)\n",
    "  y_buffer = int(kernel_buffer[1] / 2)\n",
    "\n",
    "  buffered_shape = [\n",
    "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
    "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
    "\n",
    "  imageColumns = [\n",
    "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) \n",
    "      for k in BANDS\n",
    "  ]\n",
    "    \n",
    "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
    "\n",
    "  def parse_image(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
    "\n",
    "\n",
    "  def to_tuple(inputs):\n",
    "    \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
    "    Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
    "    Args:\n",
    "    inputs: A dictionary of tensors, keyed by feature name.\n",
    "    Returns: \n",
    "    A dtuple of (inputs, outputs).\n",
    "    \"\"\"\n",
    "    inputsList = [inputs.get(key) for key in BANDS]\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    # Convert from CHW to HWC\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    return stacked\n",
    "\n",
    "  def toTupleImage(dict):\n",
    "    inputsList = [dict.get(key) for key in BANDS] #BANDS\n",
    "    stacked = tf.stack(inputsList, axis=0)\n",
    "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "    return stacked\n",
    "  \n",
    "  # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
    "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=4)\n",
    "  imageDataset = imageDataset.map(to_tuple).batch(1)\n",
    "  # Perform inference.\n",
    "  #print('Running predictions...')\n",
    "  #with tf.device('/device:GPU:0'):\n",
    "  try: \n",
    "     predictions = m.predict(imageDataset, steps=patches, verbose=2)\n",
    "  except:\n",
    "     print(f'Grid com problema:{region_id} para o ano:{index_in} \\n\\n')\n",
    "     return None\n",
    "    \n",
    "  patchesPerRow  = mixer['patchesPerRow']\n",
    "  TotalPatches   = mixer['totalPatches']\n",
    "  patchDimension = mixer['patchDimensions']\n",
    "\n",
    "  counter         = 1\n",
    "  rowCounter      = 1\n",
    "  globalCounter   = 0\n",
    "  finalArray      = numpy.array([])\n",
    "  \n",
    "  rowArray        = numpy.array([])\n",
    "  rasterFolder    = OUTPUT_PATH+'/classifications_tiff/'+str(index_in)\n",
    "  out_image_asset = user_folder + '_' + out_image_base\n",
    "  creatDirectory(rasterFolder) \n",
    "  \n",
    "  if True:\n",
    "      for raw_record in predictions:\n",
    "          raw_record = numpy.squeeze(raw_record)\n",
    "          rows,cols = raw_record.shape\n",
    "          raw_record = raw_record[128:384,128:384]\n",
    "          if rowCounter == 1:\n",
    "              finalArray = rowArray\n",
    "          if counter <= patchesPerRow:\n",
    "              if counter == 1:\n",
    "                  rowArray = raw_record\n",
    "              else:\n",
    "                  rowArray = numpy.concatenate((rowArray,raw_record), axis = 1)\n",
    "              counter = counter+1\n",
    "          else:\n",
    "              counter = 2\n",
    "              rowCounter = rowCounter+1\n",
    "              if numpy.array_equal(finalArray,rowArray):\n",
    "                  finalArray = rowArray\n",
    "              else:\n",
    "                  finalArray = numpy.concatenate((finalArray,rowArray),axis=0)\n",
    "              rowArray = raw_record \n",
    "          globalCounter = globalCounter+1\n",
    "      try: \n",
    "         finalArray = numpy.concatenate((finalArray,rowArray),axis=0)\n",
    "      except:\n",
    "         print(f'Grid com problema:{region_id} para o ano:{index_in} \\n\\n')\n",
    "         return None\n",
    "\n",
    "      rows,cols = finalArray.shape\n",
    "      driver    = gdal.GetDriverByName(\"GTiff\")\n",
    "\n",
    "      finalArray2  = numpy.array([finalArray])\n",
    "      rasterURI    = rasterFolder+'/UNET_v'+VERSION+'_grid_'+str(region_id)+'_year_'+str(index_in)+'.tif'\n",
    "      rasterURILZW = rasterFolder + '/outimage_'+VERSION+'_'+str(region_id)+'_'+str(index_in)+'_'+str(epochs)+'epochs_lzw.tif'\n",
    "        \n",
    "      with rasterio.open(rasterURI,'w',\n",
    "              driver=\"GTiff\",\n",
    "              height=rows,\n",
    "              width=cols,\n",
    "              count=1,\n",
    "              dtype=\"float32\",\n",
    "              crs=mixer[\"projection\"][\"crs\"],\n",
    "              transform=mixer[\"projection\"][\"affine\"][\"doubleMatrix\"],\n",
    "              nodata=\"nan\") as dataset:\n",
    "                  dataset.write(finalArray2)\n",
    "      !gdal_translate -of GTiff -co \"COMPRESS=LZW\" -co \"PREDICTOR=2\" -co \"TILED=YES\" {rasterURI} {rasterURILZW}\n",
    "      !rm {rasterURI}\n",
    "\n",
    "        \n",
    "  if True:      \n",
    "      out_image_file = rasterFolder+'/' + out_image_base + '_'+str(epochs)+'epochs.TFRecord'\n",
    "      out_image_mixer = rasterFolder+'/'+ out_image_base + '_'+str(epochs)+'epochs.json'\n",
    "      #----------------------------------TFRECORD WRITE ----------------------------\n",
    "      writer = tf.io.TFRecordWriter(out_image_file)\n",
    "      patches = 0\n",
    "      for predictionPatch in predictions:\n",
    "        #print('Writing patch ' + str(patches) + '...')\n",
    "        predictionPatch = predictionPatch[\n",
    "            x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
    "\n",
    "        # Create an example.\n",
    "        example = tf.train.Example(\n",
    "          features=tf.train.Features(\n",
    "            feature={\n",
    "              'classification': tf.train.Feature(\n",
    "                  float_list=tf.train.FloatList(\n",
    "                      value=predictionPatch.flatten()))\n",
    "            }\n",
    "          )\n",
    "        )\n",
    "        # Write the example.\n",
    "        writer.write(example.SerializeToString())\n",
    "        patches += 1\n",
    "\n",
    "      writer.close()\n",
    "      !cp {jsonFile} {out_image_mixer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features on GRID\n",
      "1311\n"
     ]
    }
   ],
   "source": [
    "# !pip install pygeoj\n",
    "import pygeoj\n",
    "kernel_buffer = [256, 256]\n",
    "image_base_name = 'allPatch_UNET_grid_'\n",
    "user_folder = 'projects/samm/Mapbiomas8/'\n",
    "grid = pygeoj.load(GRID_PATH)\n",
    "\n",
    "print(\"Total Features on GRID\")\n",
    "original = [int(geo.properties['id']) for geo in grid]\n",
    "original.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original = [int(geo.properties['id'])  for geo in grid if geo.properties['mining']==1]\n",
    "print(len(original))\n",
    "\n",
    "original.sort()\n",
    "print(original)\n",
    "\n",
    "newlist = [] # empty list to hold unique elements from the list\n",
    "duplist = [] # empty list to hold the duplicate elements from the list\n",
    "for i in original:\n",
    "    if int(i) not in newlist:\n",
    "        newlist.append(i)\n",
    "    else:\n",
    "        duplist.append(i)\n",
    "print('===========================\\n')\n",
    "print(\"List of duplicates\", duplist)\n",
    "print(\"Unique Item List\", len(newlist))\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inside_mg_ids = [\n",
    "1103, # Mari Indicou\n",
    "1434, # Julia Indicou AP\n",
    "1052, # MARI indicou\n",
    "1708, # Julia Indicou\n",
    "1657, # Julia Indicou\n",
    "  1760,\n",
    "  1964,\n",
    "  1761,\n",
    "  1812,\n",
    "  2016,\n",
    "  2118,\n",
    "  2169,\n",
    "  2220,\n",
    "  2221,\n",
    "  2119,\n",
    "  2170,\n",
    "  1711,\n",
    "  1762,\n",
    "  1967,\n",
    "  2120,\n",
    "  2222,\n",
    "  2018,\n",
    "  1763,\n",
    "  2019,\n",
    "  2070,\n",
    "  2121,\n",
    "  1968,\n",
    "  2223,\n",
    "  1866,\n",
    "  1969,\n",
    "  1867,\n",
    "  2071,\n",
    "  2020,\n",
    "  1918,\n",
    "  1765,\n",
    "  1816,\n",
    "  2021,\n",
    "  1766,\n",
    "  2072,\n",
    "  1919,\n",
    "  1970,\n",
    "  1868,\n",
    "  1715,\n",
    "  1716,\n",
    "  1869,\n",
    "  1971,\n",
    "  1767,\n",
    "  1818,\n",
    "  1920,\n",
    "  2022,\n",
    "  1921,\n",
    "  1870,\n",
    "  1819,\n",
    "  1972,\n",
    "  1768,\n",
    "  2023,\n",
    "  2074,\n",
    "  2075,\n",
    "  1922,\n",
    "  2024,\n",
    "  1973,\n",
    "  1820,\n",
    "  1871,\n",
    "  1770,\n",
    "  1974,\n",
    "  1821,\n",
    "  1822,\n",
    "  2025,\n",
    "  2123, #NOVO\n",
    "    2073, #nOVO\n",
    "    2122 #nOVO\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = \n",
    "VERSION     = '7_MG'\n",
    "#Local paths\n",
    "LOCAL_PATH  = '/mnt/storage10.2/mb9-unet-mineracao'\n",
    "OUTPUT_PATH = NFS_PATH + '/output/v' + VERSION\n",
    "creatDirectory(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import fnmatch\n",
    "import re\n",
    "# mypath = '/home/mluize/storage14/mosaics_sentinel2/2016'\n",
    "mypath = '/mnt/storage10.2/mosaics_landsat_2023'\n",
    "onlyfiles = [int(number) for file in listdir(mypath) if (isfile(join(mypath, file)) and (fnmatch.fnmatch(file, '*.json'))) \\\n",
    "             for number in re.findall(\"(\\d+)\", file[29:33])]\n",
    "\n",
    "# onlyfiles = [file[29:33] for file in listdir(mypath) if (isfile(join(mypath, file)) and (fnmatch.fnmatch(file, '*.json')))]\n",
    "onlyfiles = [file for file in listdir(mypath) if (isfile(join(mypath, file)) and (fnmatch.fnmatch(file, '*.json')))]\n",
    "\n",
    "onlyfiles_with_ids = [re.findall(r'_grid_(\\d+)_', file) for file in listdir(mypath) if (isfile(join(mypath, file)) \\\n",
    "                      and file.endswith('.json'))]\n",
    "onlyfiles_with_ids.sort()\n",
    "# print(onlyfiles_with_ids)\n",
    "# print(len(onlyfiles_with_ids))\n",
    "\n",
    "\n",
    "ids_list = [int(id) for sublist in onlyfiles_with_ids for id in sublist]\n",
    "original = [int(geo.properties['id']) for geo in grid]\n",
    "original.sort()\n",
    "ids_list.sort()\n",
    "print(original,'\\n')\n",
    "\n",
    "\n",
    "newlist = [] # empty list to hold unique elements from the list\n",
    "duplist = [] # empty list to hold the duplicate elements from the list\n",
    "# newlist = ids_list\n",
    "for i in original:\n",
    "    if int(i) not in newlist:\n",
    "        newlist.append(i)\n",
    "    else:\n",
    "        duplist.append(i)\n",
    "print(\"List of duplicates\", duplist)\n",
    "# print(\"Unique Item List\", newlist)\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the export.\n",
    "for region in grid:\n",
    "    region_id = int(region.properties['id'])\n",
    "    if int(region_id) in [1103,1434] :\n",
    "        print('Region:',int(region_id))\n",
    "      #print(region.geometry.coordinates)\n",
    "        for y in range(1985,2024): #cesar 1985 a 1987\n",
    "            doExport(image_base_name+str(region_id)+str('_' + MOSAIC_VERSION),y, kernel_buffer, region.geometry.coordinates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "import time\n",
    "image_base_name           = 'allPatch_UNET_grid_'\n",
    "image_base_name_for_2023  = 'mosaico_mb9_landsat_2023_allPatch_UNET_grid_'\n",
    "MOSAIC_VERSION_after_2020 = '1' \n",
    "MOSAIC_VERSION            = '4' \n",
    "\n",
    "for y in range(2022, 2023):\n",
    "    # if y == 2023: \n",
    "    #     image_base_name = image_base_name_for_2023\n",
    "    # if y > 2020:\n",
    "    #     MOSAIC_VERSION = MOSAIC_VERSION_after_2020\n",
    "    start = time.time()\n",
    "    # print('starting...')\n",
    "    # !echo 'year={y}' >> log.log\n",
    "    index = 1\n",
    "    processed_grids = []\n",
    "    for region in grid:\n",
    "        region_id = region.properties['id']\n",
    "        region_id = int(region_id)\n",
    "        # if (int(region_id) not in processed_grids) and region.properties['mining'] == 1:\n",
    "        if region.properties['mining'] == 1 and region_id not in inside_mg_ids:\n",
    "            processed_grids.append(region_id)\n",
    "            print(region_id,y)\n",
    "            user_folder = 'projects/solvedltda/assets/MB8/Mining/unet_'+VERSION+'_'+str(y)+'_lzw'\n",
    "            doPrediction(image_base_name+str(region_id)+str('_' + MOSAIC_VERSION),y,region_id, user_folder, kernel_buffer, region.geometry.coordinates,EPOCH)\n",
    "            index = index+1\n",
    "    end = time.time()\n",
    "    print('Prediction Time per year = '+str(end - start))\n",
    "print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
