{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b211a17e-111c-4423-ab5f-bbe123f785b5",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141f91b-09c0-495b-9920-077b75fa0eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:14:54.892179Z",
     "iopub.status.busy": "2025-05-15T15:14:54.891732Z",
     "iopub.status.idle": "2025-05-15T15:14:59.644535Z",
     "shell.execute_reply": "2025-05-15T15:14:59.643851Z",
     "shell.execute_reply.started": "2025-05-15T15:14:54.892151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import tensorflow as tf\n",
    "import ee\n",
    "import numpy as np\n",
    "from osgeo import ogr\n",
    "from osgeo import gdal\n",
    "import glob,pygeoj\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import folium\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "logging.getLogger('googleapicliet.discovery_cache').setLevel(logging.ERROR)\n",
    "\n",
    "gpu_dict    = {'4090':{'GPU_AFFINTY' : 0, 'GPU_MEMORY_LIMIT_GB':12}}\n",
    "sel_gpu     = '4090'\n",
    "GPU_AFFINTY = gpu_dict[sel_gpu]['GPU_AFFINTY'] \n",
    "GPU_MEMORY_LIMIT_GB = gpu_dict[sel_gpu]['GPU_MEMORY_LIMIT_GB']\n",
    "USER_EE_PROJECT='USER_PROJECT_ID'\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project = USER_EE_PROJECT)\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project = USER_EE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da72608-f60c-4060-ad8e-a9a5a49a047a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:03.793986Z",
     "iopub.status.busy": "2025-05-15T15:15:03.793314Z",
     "iopub.status.idle": "2025-05-15T15:15:03.935220Z",
     "shell.execute_reply": "2025-05-15T15:15:03.934850Z",
     "shell.execute_reply.started": "2025-05-15T15:15:03.793948Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.15.0\n",
      "Folium Version: 0.14.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'\n",
    "\n",
    "print('Tensorflow Version:',tf.__version__)\n",
    "print('Folium Version:',folium.__version__)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[GPU_AFFINTY], 'GPU')\n",
    "    GPU_MEMORY_LIMIT_GB = GPU_MEMORY_LIMIT_GB * 1e3\n",
    "    if GPU_MEMORY_LIMIT_GB == 0:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        tf.config.set_logical_device_configuration(gpus[GPU_AFFINTY],[tf.config.LogicalDeviceConfiguration(memory_limit=GPU_MEMORY_LIMIT_GB)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af202f17-7c68-441f-b731-6bd5031eeda9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:06.262752Z",
     "iopub.status.busy": "2025-05-15T15:15:06.262520Z",
     "iopub.status.idle": "2025-05-15T15:15:06.266093Z",
     "shell.execute_reply": "2025-05-15T15:15:06.265503Z",
     "shell.execute_reply.started": "2025-05-15T15:15:06.262733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_directory(new_folder):\n",
    "  ''' Check if any of the specified folder already exist'''\n",
    "  if not os.path.exists(new_folder):\n",
    "      print(f'lets make the directory: {new_folder}')\n",
    "      os.makedirs(new_folder)\n",
    "  else: return\n",
    "\n",
    "  def check_file_exists(paths):\n",
    "    \"\"\"Check if any of the specified paths already exist\"\"\"\n",
    "    for path in paths:\n",
    "        if os.path.exists(path):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873c34b-2040-4ccf-9d57-4147c460907b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENV Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e4a964-948e-4632-be0d-92de58891eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:08.667398Z",
     "iopub.status.busy": "2025-05-15T15:15:08.667121Z",
     "iopub.status.idle": "2025-05-15T15:15:08.677020Z",
     "shell.execute_reply": "2025-05-15T15:15:08.676407Z",
     "shell.execute_reply.started": "2025-05-15T15:15:08.667374Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General \n",
    "BIOMA      = 'pampa'\n",
    "BIOMA_DICT = {'amazonia':{'BIOMA_CODE':1,'DATASET_FLAG':'amz'},\n",
    "              'caatinga':{'BIOMA_CODE':2, 'DATASET_FLAG':'caatinga'},\n",
    "              'cerrado':{'BIOMA_CODE':3, 'DATASET_FLAG':'cerr'},\n",
    "              'mataatlantica':{'BIOMA_CODE':4, 'DATASET_FLAG':'mata'},\n",
    "              'pampa':{'BIOMA_CODE':5, 'DATASET_FLAG':'pampa', },\n",
    "              'pantanal':{'BIOMA_CODE':6, 'DATASET_FLAG':'pantanal'}}\n",
    "\n",
    "BIOMA_CODE = BIOMA_DICT[BIOMA]['BIOMA_CODE']\n",
    "BIOME_DATASET_FLAG = BIOMA_DICT[BIOMA]['DATASET_FLAG']\n",
    "\n",
    "VERSION        = '1'\n",
    "MOSAIC_VERSION = '1'\n",
    "\n",
    "BUCKET = 'mb10-mining'\n",
    "GDRIVE = 'mb10-unet-mineracao-'+BIOMA\n",
    "FOLDER = 'training_samples'\n",
    "TRAINING_BASE = 'training_patches_'+MOSAIC_VERSION\n",
    "EVAL_BASE     = 'eval_patches_'+MOSAIC_VERSION\n",
    "\n",
    "#Local paths\n",
    "LOCAL_PATH  = '~/mb10-unet-mineracao/'+BIOMA\n",
    "MODEL_DIR   = LOCAL_PATH+'/checkpoint/v'+VERSION\n",
    "NFS_PATH = '~/mb10-unet-mineracao/'+BIOMA\n",
    "OUTPUT_PATH = NFS_PATH+'/output/v'+VERSION\n",
    "\n",
    "create_directory(MODEL_DIR)\n",
    "create_directory(OUTPUT_PATH)\n",
    "\n",
    "# Exportation Configs\n",
    "BUCKET_patch = BUCKET\n",
    "FOLDER_patch = 'allPatch'\n",
    "FOLDER_classification = 'mb10_mining_'+VERSION\n",
    "\n",
    "# Specify inputs (Landsat bands) to the model and the response variable.\n",
    "opticalBands   = ['green','red','nir','swir1','NDVI','MNDWI']\n",
    "\n",
    "BANDS    = opticalBands\n",
    "RESPONSE = 'supervised'\n",
    "FEATURES = BANDS + [RESPONSE]\n",
    "\n",
    "# Specify the size and shape of patches expected by the model.\n",
    "KERNEL_SIZE  = 256\n",
    "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
    "\n",
    "# Sizes of the training and evaluation datasets.\n",
    "TRAIN_SIZE = 0\n",
    "EVAL_SIZE = 0\n",
    "\n",
    "# Specify model training parameters.\n",
    "BATCH_SIZE  = 16\n",
    "DROPOUT     = 0.3\n",
    "EPOCHS      = 50\n",
    "BUFFER_SIZE = 1000\n",
    "OPTIMIZER   = 'Nadam' \n",
    "LOSS        = 'BinaryCrossentropy'\n",
    "METRICS     = ['BinaryIoU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2056915-e816-4dcd-b213-005e7a695e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:29:53.316096Z",
     "iopub.status.busy": "2025-04-30T16:29:53.315809Z",
     "iopub.status.idle": "2025-04-30T16:29:57.023664Z",
     "shell.execute_reply": "2025-04-30T16:29:57.023007Z",
     "shell.execute_reply.started": "2025-04-30T16:29:53.316072Z"
    }
   },
   "outputs": [],
   "source": [
    "mosaic_year   = 2022\n",
    "\n",
    "supervisedImg = ee.Image('projects/mapbiomas-workspace/COLECAO9/mineracao/'+str(mosaic_year)+'-5').gte(1).unmask(0).rename(RESPONSE)\n",
    "supervisedChannel = supervisedImg.toByte().rename(RESPONSE);\n",
    "\n",
    "image = ee.Image('projects/mapbiomas-workspace/TRANSVERSAIS/ZONACOSTEIRA6/mosaic_'+str(mosaic_year)).addBands(supervisedChannel)\n",
    "\n",
    "mapid = image.getMapId({'bands': ['swir1', 'nir', 'red'], 'min': 30, 'max': 150})\n",
    "map = folium.Map(location=[-5.9442, -56.5265])\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Planet',\n",
    "    overlay=True,\n",
    "    name='Mosaic composite',\n",
    "  ).add_to(map)\n",
    "mapid = supervisedChannel.select(RESPONSE).getMapId({'min': 0, 'max': 1, 'pallete':'#ff0000'})\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='supervisedLayer',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392b454-0219-4bb1-9dc1-6b9c39e1235e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:30:00.238395Z",
     "iopub.status.busy": "2025-04-30T16:30:00.238121Z",
     "iopub.status.idle": "2025-04-30T16:30:00.243709Z",
     "shell.execute_reply": "2025-04-30T16:30:00.243054Z",
     "shell.execute_reply.started": "2025-04-30T16:30:00.238371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureStack = ee.Image.cat([\n",
    "  image.select(BANDS).unmask(0),\n",
    "  image.select(RESPONSE).unmask(0)\n",
    "]).float()\n",
    "    \n",
    "list = ee.List.repeat(1, KERNEL_SIZE)\n",
    "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
    "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
    "\n",
    "arrays = featureStack.neighborhoodToArray(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15cbc66-d51f-4473-a7a0-f15844dafa0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:30:17.498160Z",
     "iopub.status.busy": "2025-04-30T16:30:17.497869Z",
     "iopub.status.idle": "2025-04-30T16:30:18.173585Z",
     "shell.execute_reply": "2025-04-30T16:30:18.172902Z",
     "shell.execute_reply.started": "2025-04-30T16:30:17.498136Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingPolys =  ee.FeatureCollection.loadBigQueryTable('solved-mb10.mb10_database.amostras_'+str(BIOME_DATASET_FLAG)+'_treino','geo')\n",
    "evalPolys = ee.FeatureCollection.loadBigQueryTable('solved-mb10.mb10_database.amostras_'+str(BIOME_DATASET_FLAG)+'_teste','geo')\n",
    "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(evalPolys, 2)\n",
    "polyImage = polyImage.updateMask(polyImage)\n",
    "\n",
    "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
    "map = folium.Map(location=[-1.3621, -45.2738], zoom_start=5)\n",
    "\n",
    "map.add_child(folium.LayerControl())\n",
    "folium.TileLayer(\n",
    "    tiles=mapid['tile_fetcher'].url_format,\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='training polygons',\n",
    "  ).add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BASE= 'training_patches_'+str(VERSION)+'_'+BIOMA\n",
    "EVAL_BASE = 'eval_patches_'+str(VERSION)+'_'+BIOMA\n",
    "FOLDER_TRAIN = 'training_samples_v'+str(VERSION)+'_MB10_'+BIOMA\n",
    "FOLDER_EVAL = 'eval_samples_v'+str(VERSION)+'_MB10_'+BIOMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcde065-8448-4f55-b0b8-8984aa1d3dfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train/Test Chips Exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6fb5b-e430-485a-b762-f3582ec1db10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T16:31:34.687030Z",
     "iopub.status.busy": "2025-04-30T16:31:34.686742Z",
     "iopub.status.idle": "2025-04-30T16:33:59.993603Z",
     "shell.execute_reply": "2025-04-30T16:33:59.993049Z",
     "shell.execute_reply.started": "2025-04-30T16:31:34.687006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the feature collections to lists for iteration.\n",
    "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
    "evalPolysList = evalPolys.toList(evalPolys.size())\n",
    "# These numbers determined experimentally.\n",
    "n = 20 # Number of shards in each polygon.\n",
    "N = 200 # Total sample size in each polygon.\n",
    "\n",
    "#Add some generalism\n",
    "TRAIN_SIZE = trainingPolys.size().getInfo()*N\n",
    "EVAL_SIZE = evalPolys.size().getInfo()*N\n",
    "print('TRAIN:'+str(TRAIN_SIZE))\n",
    "print('EVAL:'+str(EVAL_SIZE))\n",
    "\n",
    "# Export all the training data (in many pieces), with one task \n",
    "# per geometry.\n",
    "for g in range(trainingPolys.size().getInfo()):\n",
    "  geomSample = ee.FeatureCollection([])\n",
    "  for i in range(n):\n",
    "    sample = arrays.sample(\n",
    "      region = ee.Feature(trainingPolysList.get(g)).geometry(), \n",
    "      scale = 30, \n",
    "      numPixels = N / n, # Size of the shard.\n",
    "      seed = i,\n",
    "      tileScale = 8\n",
    "    )\n",
    "    geomSample = geomSample.merge(sample)\n",
    "  \n",
    "  desc = TRAINING_BASE + '_g' + str(g)\n",
    "  task = ee.batch.Export.table.toDrive(\n",
    "    collection = geomSample,\n",
    "    description = desc, \n",
    "    folder = GDRIVE+'_'+FOLDER_TRAIN, \n",
    "    fileNamePrefix = desc,\n",
    "    fileFormat = 'TFRecord',\n",
    "    selectors = BANDS + [RESPONSE]\n",
    "  )\n",
    "  task.start()\n",
    "\n",
    "# Export all the evaluation data.\n",
    "for g in range(evalPolys.size().getInfo()):\n",
    "  geomSample = ee.FeatureCollection([])\n",
    "  for i in range(n):\n",
    "    sample = arrays.sample(\n",
    "      region = ee.Feature(evalPolysList.get(g)).geometry(), \n",
    "      scale = 30, \n",
    "      numPixels = N / n,\n",
    "      seed = i,\n",
    "      tileScale = 8\n",
    "    )\n",
    "    geomSample = geomSample.merge(sample)\n",
    "  \n",
    "  desc = EVAL_BASE + '_g' + str(g)\n",
    "  task = ee.batch.Export.table.toDrive(\n",
    "    collection = geomSample,\n",
    "    description = desc, \n",
    "    folder = GDRIVE+'_'+FOLDER_EVAL, \n",
    "    fileNamePrefix = desc,\n",
    "    fileFormat = 'TFRecord',\n",
    "    selectors = BANDS + [RESPONSE],\n",
    "  )\n",
    "  task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5b575-9da1-41ce-878d-8fa8973835bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T01:31:38.272277Z",
     "iopub.status.busy": "2025-05-02T01:31:38.271684Z",
     "iopub.status.idle": "2025-05-02T01:31:38.679954Z",
     "shell.execute_reply": "2025-05-02T01:31:38.679329Z",
     "shell.execute_reply.started": "2025-05-02T01:31:38.272251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "  Returns: \n",
    "    A dictionary of tensors, keyed by feature name.\n",
    "  \"\"\"\n",
    "  print(FEATURES_DICT)\n",
    "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "\n",
    "\n",
    "def to_tuple(inputs):\n",
    "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
    "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
    "  Args:\n",
    "    inputs: A dictionary of tensors, keyed by feature name.\n",
    "  Returns: \n",
    "    A dtuple of (inputs, outputs).\n",
    "  \"\"\"\n",
    "  inputsList = [inputs.get(key) for key in FEATURES]\n",
    "  stacked = tf.stack(inputsList, axis=0)\n",
    "  # Convert from CHW to HWC\n",
    "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
    "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
    "\n",
    "\n",
    "def get_dataset(pattern):\n",
    "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
    "  Get all the files matching the pattern, parse and convert to tuple.\n",
    "  Args:\n",
    "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
    "  Returns: \n",
    "    A tf.data.Dataset\n",
    "  \"\"\"\n",
    "  glob = tf.io.gfile.glob(pattern)\n",
    "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
    "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413411d9-c828-4457-8481-0be6000c423e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T01:55:54.426185Z",
     "iopub.status.busy": "2025-05-02T01:55:54.425891Z",
     "iopub.status.idle": "2025-05-02T01:55:54.476751Z",
     "shell.execute_reply": "2025-05-02T01:55:54.476134Z",
     "shell.execute_reply.started": "2025-05-02T01:55:54.426160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "import random\n",
    "\n",
    "TRAIN_SIZE = 0\n",
    "\n",
    "def augment_spatial(image, label):\n",
    "    \"\"\"Randomly translates/pads the image with a small tilt effect.\"\"\"\n",
    "    padded_image = tf.pad(image, [[64, 64], [64, 64], [0, 0]], mode='CONSTANT') \n",
    "    padded_label = tf.pad(label, [[64, 64], [64, 64], [0, 0]], mode='CONSTANT')\n",
    "    random_x = random.randint(0, 128)\n",
    "    random_y = random.randint(0, 128)\n",
    "    cropped_image = tf.slice(padded_image, [random_x,random_y, 0], [256, 256, 6])\n",
    "    cropped_label = tf.slice(padded_label, [random_x, random_y, 0], [256, 256, 1])\n",
    "    return cropped_image,cropped_label\n",
    "\n",
    "\n",
    "def get_training_dataset():\n",
    "    TRAINING_BASE = \"training_patches_1\"\n",
    "    glob = '~/train/*.tfrecord.gz'\n",
    "    dataset = get_dataset(glob)\n",
    "    TRAIN_SIZE = dataset.reduce(np.int64(0), lambda x,_ : x + 1).numpy()\n",
    "    augmented_dataset = dataset.map(augment_spatial, num_parallel_calls=100)\n",
    "    dataset = augmented_dataset.concatenate(dataset).shuffle(20000, reshuffle_each_iteration=True).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "training = get_training_dataset()\n",
    "first_element = next(iter(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef8b20-e422-4fc8-8f32-f62a9e7b380e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T01:56:50.651400Z",
     "iopub.status.busy": "2025-05-02T01:56:50.651106Z",
     "iopub.status.idle": "2025-05-02T01:56:50.698063Z",
     "shell.execute_reply": "2025-05-02T01:56:50.697245Z",
     "shell.execute_reply.started": "2025-05-02T01:56:50.651376Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_SIZE = 0\n",
    "\n",
    "def augment_spatial_eval(image, label):\n",
    "    \"\"\"Randomly translates/pads the image with a small tilt effect.\"\"\"\n",
    "    padded_image = tf.pad(image, [[64, 64], [64, 64], [0, 0]], mode='CONSTANT')\n",
    "    padded_label = tf.pad(label, [[64, 64], [64, 64], [0, 0]], mode='CONSTANT')\n",
    "    random_x = random.randint(0, 128)\n",
    "    random_y = random.randint(0, 128)\n",
    "    cropped_image = tf.slice(padded_image, [random_x,random_y, 0], [256, 256, 6])\n",
    "    cropped_label = tf.slice(padded_label, [random_x, random_y, 0], [256, 256, 1])\n",
    "    return cropped_image,cropped_label\n",
    "\n",
    "\n",
    "def get_eval_dataset():\n",
    "    EVAL_BASE = 'eval_patches_1'\n",
    "    glob = '~/eval/*.tfrecord.gz'\n",
    "    dataset = get_dataset(glob)\n",
    "    EVAL_SIZE = dataset.reduce(np.int64(0), lambda x,_ : x + 1).numpy()\n",
    "    augmented_dataset = dataset.map(augment_spatial, num_parallel_calls=100)\n",
    "    dataset = augmented_dataset.concatenate(dataset).shuffle(6556*2, reshuffle_each_iteration=True).map(augment_spatial_eval, num_parallel_calls=100).batch(2).repeat()\n",
    "    return dataset\n",
    "\n",
    "evaluation = get_eval_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b905466-9637-40b1-a030-6624f28cb52b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4e7438-4903-4eb1-a7c6-2673a9bd7775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:12.712853Z",
     "iopub.status.busy": "2025-05-15T15:15:12.712412Z",
     "iopub.status.idle": "2025-05-15T15:15:12.740347Z",
     "shell.execute_reply": "2025-05-15T15:15:12.739812Z",
     "shell.execute_reply.started": "2025-05-15T15:15:12.712826Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, num_filters):\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "\tencoder = layers.BatchNormalization()(encoder)\n",
    "\tencoder = layers.Activation('relu')(encoder)\n",
    "\treturn encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "\tencoder = conv_block(input_tensor, num_filters)\n",
    "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "\treturn encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "\tdecoder = layers.BatchNormalization()(decoder)\n",
    "\tdecoder = layers.Activation('relu')(decoder)\n",
    "\treturn decoder\n",
    "\n",
    "def get_model():\n",
    "\tinputs = layers.Input(shape=[None, None, len(BANDS)])\n",
    "\tencoder0_pool, encoder0 = encoder_block(inputs, 64) \n",
    "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 128)\n",
    "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 256)\n",
    "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 512)\n",
    "\tcenter = conv_block(encoder3_pool, 1024)\n",
    "\tdecoder4 = decoder_block(center, encoder3, 512)\n",
    "\tdecoder3 = decoder_block(decoder4, encoder2, 256)\n",
    "\tdecoder2 = decoder_block(decoder3, encoder1, 128)\n",
    "\tdecoder1 = decoder_block(decoder2, encoder0, 64)\n",
    "\n",
    "\tdropout = layers.Dropout(DROPOUT, name=\"dropout\", noise_shape=None, seed=None)(decoder1)\n",
    "\toutputs = layers.Conv2D(1, (1, 1),  activation=tf.nn.sigmoid, padding='same', kernel_initializer=tf.keras.initializers.GlorotNormal())(dropout)\n",
    "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\t\n",
    "\toptimizer = tf.keras.optimizers.Nadam( learning_rate=0.000005, name='optimizer')\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=optimizer, \n",
    "\t\tloss=losses.get(LOSS),\n",
    "\t\tmetrics=[metrics.get(metric) for metric in METRICS]\n",
    "    )\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687c0e4-5db1-4ebb-84aa-52e633a03964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:17.752471Z",
     "iopub.status.busy": "2025-05-15T15:15:17.752182Z",
     "iopub.status.idle": "2025-05-15T15:15:42.605946Z",
     "shell.execute_reply": "2025-05-15T15:15:42.605446Z",
     "shell.execute_reply.started": "2025-05-15T15:15:17.752447Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = get_model()\n",
    "EPOCH = 0\n",
    "CHECK_MODEL_DIR_MG = '~/checkpoint/v1/cp-00'+str(EPOCH)+'.keras'\n",
    "m.load_weights(CHECK_MODEL_DIR_MG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe22f2-a4a5-45bf-89a2-e2af9e1af71f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T01:33:08.382580Z",
     "iopub.status.busy": "2025-05-02T01:33:08.382254Z",
     "iopub.status.idle": "2025-05-02T01:33:08.388632Z",
     "shell.execute_reply": "2025-05-02T01:33:08.388070Z",
     "shell.execute_reply.started": "2025-05-02T01:33:08.382559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def previewClass(epoch,log):\n",
    "    \"\"\"Callback funtion for training acompaniment by visual verification\"\"\"\n",
    "    counter = 0\n",
    "    for batch in evaluation.shuffle(1000).take(3):\n",
    "        pureImage = batch[0]\n",
    "        supervised = batch[1]\n",
    "        stacked = tf.transpose(pureImage[0], [0, 1, 2]).numpy()\n",
    "        stackedS = tf.transpose(supervised[0], [0, 1, 2]).numpy()\n",
    "\n",
    "        test_pred_raw = m.predict(pureImage)\n",
    "        test_pred_raw = tf.transpose(test_pred_raw[0],[0, 1, 2]).numpy()\n",
    "        fig = plt.figure(figsize=[12,4])\n",
    "        # show original image\n",
    "        fig.add_subplot(131)\n",
    "        plt.imshow(stacked[:,:,0:3].astype(np.uint8), interpolation='nearest', vmin=0, vmax=255)\n",
    "        fig.add_subplot(132)\n",
    "        plt.imshow(stackedS[:,:,0], interpolation='nearest',cmap=\"gray\")\n",
    "        fig.add_subplot(133)\n",
    "        plt.imshow(test_pred_raw[:,:,0], interpolation='nearest',cmap=\"gray\")\n",
    "        plt.show()\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db056586-7287-4090-a4a9-6fa5504cec52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-02T17:34:54.437696Z",
     "iopub.status.busy": "2025-05-02T17:34:54.437301Z",
     "iopub.status.idle": "2025-05-02T19:32:59.704454Z",
     "shell.execute_reply": "2025-05-02T19:32:59.703722Z",
     "shell.execute_reply.started": "2025-05-02T17:34:54.437662Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "checkpoint_path = MODEL_DIR+\"/cp-{epoch:04d}.keras\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='output/v'+VERSION+'/log_model',write_images=True)\n",
    "\n",
    "cp_callback  = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,verbose=1, save_weights_only=False,save_best_only=False,save_freq='epoch')\n",
    "img_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=previewClass)\n",
    "\n",
    "\n",
    "result = m.fit(x=training,\n",
    "  epochs=100,\n",
    "  initial_epoch=0, # REMEBER TO CHANGE THIS INITIAL EPOCH PARAM, WHEN OTHER MODEL HAS BEEN LOADED\n",
    "  steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE), \n",
    "  verbose=1,\n",
    "  shuffle=True,\n",
    "  validation_data=evaluation,\n",
    "  validation_steps=EVAL_SIZE,\n",
    "  callbacks = [cp_callback,tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29146e05",
   "metadata": {},
   "source": [
    "# Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5c6c4",
   "metadata": {},
   "source": [
    "# Export mosaics to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doExport(out_image_base,year,region_id, kernel_buffer, roi):\n",
    "  \"\"\"Run the image export task.\"\"\"\n",
    "  image = ee.Image('projects/'+USER_EE_PROJECT+'/assets/USER_PATH/mosaic_'+str(year))\n",
    "  # Export the image, specifying scale and region.\n",
    "  task = ee.batch.Export.image.toDrive(\n",
    "    image          = image.select(BANDS).toFloat(),\n",
    "    description    = out_image_base+'_'+str(year),\n",
    "    fileNamePrefix = out_image_base+'_'+str(year), \n",
    "    folder         = 'mosaics_landsat/'+str(year),\n",
    "    scale          = 30,\n",
    "    region         = roi,\n",
    "    fileFormat     = 'GEOTIFF',\n",
    "    formatOptions  = { \n",
    "      'patchDimensions': KERNEL_SHAPE,\n",
    "      'kernelSize': kernel_buffer,\n",
    "      'compressed': True,\n",
    "      'maxFileSize': 157286400\n",
    "    }\n",
    "  )\n",
    "  task.start()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66132bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the export.\n",
    "for region in full_aqua_regions:\n",
    "    region_id = int(region.properties['id'])\n",
    "    image_base_name =f'v{MOSAIC_VERSION}_L9_grid_{region_id}'\n",
    "    if int(region_id):\n",
    "      print('Region:',int(region_id))\n",
    "      for y in range(1985, 2021): \n",
    "          doExport(image_base_name,y, kernel_buffer, region.geometry.coordinates[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba3ab3-9036-4957-9f13-b7c086b01f9a",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d29a617-1465-4f33-ba1c-25c498f2790d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:42.607053Z",
     "iopub.status.busy": "2025-05-15T15:15:42.606864Z",
     "iopub.status.idle": "2025-05-15T15:15:43.065790Z",
     "shell.execute_reply": "2025-05-15T15:15:43.065203Z",
     "shell.execute_reply.started": "2025-05-15T15:15:42.607038Z"
    }
   },
   "outputs": [],
   "source": [
    "def mosaic_predict(mosaic_lzw,year, region_id, output_path, version, kernel_dim, optical_bands, optical_indices, model, mosaic_scale):\n",
    "    \"\"\"Executes segmentation over mosaic data exported from EE as .geotiff\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mosaic_lzw : geotiff\n",
    "        Geotiff data representing the LANDSAT mosaic. The region exported must be the same as the region_id\n",
    "    year : int\n",
    "        The year of said geotiff\n",
    "    region_id : int\n",
    "        The id of the geojson grid that identifies the geotiff region\n",
    "    output_path: str\n",
    "        The output path for the segmented data\n",
    "    version: int\n",
    "        The segmentation version\n",
    "    kernel_dim: int\n",
    "        The dimention of the patches to be segmented\n",
    "    optical_bands: list\n",
    "        List of optical bands present on each geotiff\n",
    "    optical_indices: list\n",
    "        List of optical indices on each geotiff\n",
    "    model: Tensorflow model\n",
    "        The model trained\n",
    "    mosaic_scale: int\n",
    "        The scale of the geotiff. Tipically, for landsat the scale is 30\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Geotiff\n",
    "        A geotiff corresonding to the segmentation fo the target for the mosaic_lzw\n",
    "    \"\"\"\n",
    "    paths_to_check = [\n",
    "\n",
    "        f'{output_path}/{year}/{EPOCH}/outimage_v{version}_e{EPOCH}_grid_{region_id}_{year}_lzw.tif',\n",
    "        f'{output_path}/{year}/{EPOCH}/outimage_v{version}_e{EPOCH}_grid_{region_id}_{year}_byte_lzw.tif',\n",
    "        f'{output_path}/{year}/{EPOCH}/outimage_v{version}_e{EPOCH}_grid_{region_id}_{year}_byte_lzw2.tif',\n",
    "    ]\n",
    "    if check_file_exists(paths_to_check):\n",
    "        return\n",
    "\n",
    "    with rasterio.open(mosaic_lzw, 'r') as ds:\n",
    "        arr = ds.read()\n",
    "    arr = np.clip(arr, 0, None)\n",
    "    \n",
    "    img_arr_original = arr.astype(np.float32)\n",
    "    if img_arr_original.shape[0] > 6:\n",
    "        img_arr_original = img_arr_original[1:, :, :]\n",
    "    img_arr_original = np.nan_to_num(img_arr_original, nan=0.0) \n",
    "\n",
    "    \n",
    "    iit = np.array(np.transpose(img_arr_original,[1,2,0]))\n",
    "\n",
    "    ''' MAKE THE IMAGE QUADRATIC '''\n",
    "    arr_shape_xy = np.array(img_arr_original.shape[1:])\n",
    "    min_dim = arr_shape_xy.min()\n",
    "    index_min_dim = np.where(arr_shape_xy==min_dim)[0][0]\n",
    "    if index_min_dim == 0: \n",
    "        img_arr = img_arr_original[:, :, :min_dim] \n",
    "    elif index_min_dim ==1:  \n",
    "        img_arr = img_arr_original[:, :min_dim, :]\n",
    "    \n",
    "    ''' ENSURE IMAGE DIMENSIONS ARE MULTIPLES OF kernel_dim '''\n",
    "    pad_size = (kernel_dim - (min_dim % kernel_dim)) % kernel_dim \n",
    "    \n",
    "    if pad_size > 0:\n",
    "        img_arr = np.pad(img_arr, ((0, 0), (0, pad_size), (0, pad_size)), mode='edge')\n",
    "\n",
    "    ''' FULL BANDS (6) '''\n",
    "    disired_dims    = kernel_dim*2 \n",
    "    bands           = optical_bands + optical_indices\n",
    "    patches         = patchify(img_arr, (len(bands), disired_dims, disired_dims), step=kernel_dim)\n",
    "    dim             = patches.shape[1]\n",
    "    patch2          = patches.reshape((1, dim**2, len(bands), disired_dims, disired_dims))\n",
    "    patch2_reshaped = patch2[0].reshape((dim**2, len(bands), disired_dims, disired_dims))\n",
    "    patch3          = np.transpose(patch2_reshaped, [0,2,3,1])\n",
    "\n",
    "    curr_patch      = tf.data.Dataset.from_tensor_slices(patch3).batch(1)\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        predictions_arr = model.predict(curr_patch, batch_size=8,steps=None, verbose=1)\n",
    "    patchesPerRow  = dim \n",
    "    TotalPatches   = dim**2 \n",
    "    patchDimension = [disired_dims,disired_dims] \n",
    "\n",
    "    counter       = 1\n",
    "    rowCounter    = 1\n",
    "    globalCounter = 0\n",
    "    finalArray    = np.array([])\n",
    "    rowArray      = np.array([])\n",
    "\n",
    "    for raw_record in predictions_arr:\n",
    "        raw_record = np.squeeze(raw_record) \n",
    "        rows,cols = raw_record.shape\n",
    "        \n",
    "        limite_esquerda = kernel_dim//2               \n",
    "        limite_direita  = kernel_dim + (kernel_dim//2)\n",
    "        limite_inferior = kernel_dim + (kernel_dim//2)\n",
    "        limite_superior = kernel_dim//2               \n",
    "        if rowCounter == 1: \n",
    "            limite_superior = 0 \n",
    "        if (counter == 1) or (counter == patchesPerRow+1):\n",
    "            limite_esquerda = 0\n",
    "        if (counter == patchesPerRow+1) and rowCounter == 1:\n",
    "            limite_superior = kernel_dim//2\n",
    "\n",
    "        if counter == patchesPerRow:\n",
    "            limite_direita = kernel_dim * 2\n",
    "\n",
    "        if rowCounter == (TotalPatches/patchesPerRow) or (rowCounter == (TotalPatches/patchesPerRow)-1 and counter == patchesPerRow+1):\n",
    "            limite_inferior = kernel_dim * 2\n",
    "\n",
    "        raw_record = raw_record[limite_superior:limite_inferior,limite_esquerda:limite_direita]\n",
    "        if rowCounter == 1:\n",
    "            finalArray = rowArray\n",
    "        if counter <= patchesPerRow: \n",
    "            if counter == 1:\n",
    "                rowArray = raw_record\n",
    "            else:\n",
    "                rowArray = np.concatenate((rowArray,raw_record), axis = 1)\n",
    "            counter = counter+1\n",
    "        else:\n",
    "            counter = 2\n",
    "            rowCounter = rowCounter+1\n",
    "            if np.array_equal(finalArray,rowArray):\n",
    "                finalArray = rowArray\n",
    "            else:\n",
    "                finalArray = np.concatenate((finalArray,rowArray),axis=0)\n",
    "            rowArray = raw_record\n",
    "        globalCounter = globalCounter+1\n",
    "    finalArray = np.concatenate((finalArray,rowArray),axis=0)\n",
    "    finalArray_padded = dynamic_slice_or_pad(arr_shape_xy, finalArray)\n",
    "    threshold      = 0.1\n",
    "    rows,cols = finalArray_padded.shape\n",
    "    finalArray_padded = np.array([finalArray_padded]) \n",
    "    output_path = f'{output_path}/{year}/{EPOCH}'\n",
    "    create_directory(output_path)\n",
    "    raster_uri     = output_path + '/UNET_v'+version+'grid'+str(region_id)+'_'+str(year)+'.tif'\n",
    "    try:\n",
    "        if not np.any(np.isnan(finalArray_padded)):\n",
    "            finalArray_padded = np.round((finalArray_padded.astype(np.float32))*255).astype(np.uint8)\n",
    "        raster_uri_lzw = f'{output_path}/outimage_v'+version+'_e'+str(EPOCH)+'_grid_'+str(region_id)+'_'+str(year)+'_byte_lzw.tif'\n",
    "        data_type = \"uint8\"\n",
    "    except Exception as inst:\n",
    "        print(f'ERROR: For grid {region_id} \\n {inst.args}, {inst}\\n Raster in float')\n",
    "        raster_uri_lzw = f'{output_path}/outimage_v'+version+'_e'+str(EPOCH)+'_grid_'+str(region_id)+'_'+str(year)+'_float_lzw.tif'\n",
    "        data_type = \"float32\"  \n",
    "\n",
    "    with rasterio.open(raster_uri,'w',\n",
    "                  driver=\"GTiff\",\n",
    "                  height=rows,\n",
    "                  width=cols,\n",
    "                  count=1,\n",
    "                  dtype=data_type,\n",
    "                  crs='EPSG:4326',\n",
    "                  transform=ds.transform,\n",
    "                  nodata=0) as dataset:\n",
    "                      dataset.write(finalArray_padded)\n",
    "    dataset = gdal.Open(raster_uri, gdal.GA_Update)\n",
    "    !gdal_translate -of GTiff -ot Byte -co \"COMPRESS=LZW\" -co \"PREDICTOR=2\" -co \"TILED=YES\" {raster_uri} {raster_uri_lzw} \n",
    "    !rm {raster_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aee0320-e014-4592-ad2e-732c7fb91451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T15:15:44.892092Z",
     "iopub.status.busy": "2025-05-15T15:15:44.891809Z",
     "iopub.status.idle": "2025-05-15T15:15:44.898476Z",
     "shell.execute_reply": "2025-05-15T15:15:44.897818Z",
     "shell.execute_reply.started": "2025-05-15T15:15:44.892068Z"
    }
   },
   "outputs": [],
   "source": [
    "def dynamic_slice_or_pad(target_shape, predicted_img):\n",
    "\n",
    "    \"\"\"Add padding to the segmentation output so that it mayches the mosaic geotiff input dimentions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_shape : list\n",
    "        List with the size of total rows and columns of the input image\n",
    "    predicted_img : Numpy array\n",
    "        Array repreenting the segmented output\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy array\n",
    "        Numpy array representing the segmentation output properly padded\n",
    "    \"\"\"\n",
    "    target_rows, target_cols = target_shape\n",
    "    pad_rows = target_rows - predicted_img.shape[0]\n",
    "    pad_cols = target_cols - predicted_img.shape[1]\n",
    "\n",
    "    if pad_rows < 0:\n",
    "        predicted_img = predicted_img[:target_rows, :]\n",
    "    elif pad_rows > 0:\n",
    "        predicted_img = np.pad(predicted_img, ((0, pad_rows), (0, 0)), mode='constant')\n",
    "\n",
    "    if pad_cols < 0:\n",
    "        predicted_img = predicted_img[:, :target_cols]\n",
    "    elif pad_cols > 0:\n",
    "        predicted_img = np.pad(predicted_img, ((0, 0), (0, pad_cols)), mode='constant')\n",
    "    \n",
    "    return predicted_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30f4c8-670e-4319-a480-db36d10368fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T19:36:12.405785Z",
     "iopub.status.busy": "2025-05-15T19:36:12.405496Z",
     "iopub.status.idle": "2025-05-15T19:36:14.005994Z",
     "shell.execute_reply": "2025-05-15T19:36:14.005497Z",
     "shell.execute_reply.started": "2025-05-15T19:36:12.405761Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction iteration\n",
    "ROOT_PATH      = './'\n",
    "YEAR           = 2024\n",
    "MOSAIC_VERSION = '1'\n",
    "mosaic_scale   = 30\n",
    "\n",
    "\n",
    "GRIDS_IDS    = pygeoj.load(f'{ROOT_PATH}/GRIDS/GRID_ALLCLASSES_COLECAO_10_STACK_BIOMA_V1.geojson')\n",
    "loaded_model = m\n",
    "reduced_grid = [int(feature.properties['id']) for feature in GRIDS_IDS if feature.properties['mining'] == 1 and feature.properties['CD_Bioma'] == BIOMA_CODE ]\n",
    "reduced_grid = [int(n + 1) for n in reduced_grid]\n",
    "reduced_grid.sort()\n",
    "\n",
    "start = time.time()\n",
    "for EPOCH in range(70,80,10):\n",
    "    print(EPOCH)\n",
    "    CHECK_MODEL_DIR_MG = '~/'+str(BIOMA)+'/checkpoint/v1/cp-00'+str(EPOCH)+'.keras'\n",
    "    m.load_weights(CHECK_MODEL_DIR_MG)\n",
    "    for year in range(2020,2024):\n",
    "        print(f'/n/nYEAR:{year}')\n",
    "        i = 0\n",
    "\n",
    "        for region_id in reduced_grid:\n",
    "            print(f'REGION ID {region_id}')\n",
    "            tf.keras.backend.clear_session()\n",
    "            start = time.time()\n",
    "\n",
    "            MOSAIC_PATH  = f'{ROOT_PATH}/mosaico_landsat/{year}'\n",
    "            search_pattern = os.path.join(MOSAIC_PATH, f'v{MOSAIC_VERSION}_*_grid_{region_id}_{year}*.tif')\n",
    "            matching_files = glob.glob(search_pattern)\n",
    "            \n",
    "            if matching_files:\n",
    "                i = i+1\n",
    "                region_mosaic_file = matching_files[0]\n",
    "                mosaic_predict(region_mosaic_file, year, region_id, OUTPUT_PATH, VERSION, KERNEL_SIZE, opticalBands, [], loaded_model, mosaic_scale)\n",
    "            else:\n",
    "                print(i)\n",
    "                print(\"No matching:\",search_pattern)\n",
    "    end = time.time()\n",
    "    print('Prediction Time per year = '+str(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
